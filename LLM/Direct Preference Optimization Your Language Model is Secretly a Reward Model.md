# 【NIPS 2023】 Direct Preference Optimization: Your Language Model is Secretly a Reward Model

<<<<<<< HEAD
阅读时间：1125
=======
阅读时间：1030
斯坦福大学
>>>>>>> 1556c810a5e6dae5fd5f99822d735db99e17635f

## Motivation
语言模型策略和奖励函数之间的映射

## Idea
如何不借助奖励模型利用成对的正负样本进行偏好预测
几乎不调整超参数？